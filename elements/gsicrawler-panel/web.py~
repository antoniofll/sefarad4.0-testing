import threading
from time import sleep
import os
import json
import imp

import uuid
from bottle import route, run, template, static_file, response, request, install

pending_analysis = {}

def return_json(result):
	response.content_type = 'application/json'
	return json.dumps(result)


def startScraper(webpage, url, unique_id):
	try:
		filePath = '%s/elements/gsicrawler-panel/analysis/%s.scraper' % (os.getcwd(), unique_id)
		scraperImported = imp.load_source(webpage, '%s/scrapers/%s.py' % (os.getcwd(), webpage))
		scraperTask = threading.Thread(target=scraperImported.startScraping, args=(url, filePath))
		scraperTask.start()
	except Exception as e:
		return {'error':'%s scraper doesn\'t exist' % (webpage), 'loading':False}

def startAnalysis(scraped_reviews, analysis_type, unique_id):
	try:
		resultPath = '%s/elements/gsicrawler-panel/analysis/%s.analisis' % (os.getcwd(), unique_id)
		importedAnalyzer = imp.load_source(analysis_type, '%s/analyzers/%s.py' % (os.getcwd(), analysis_type))
		analysisTask = threading.Thread(target=importedAnalyzer.analyze, args=(scraped_reviews, resultPath))
		analysisTask.start()
		return None
	except Exception as e:
		print '###### startAnalysis ' + str(e)
		return {'error':'%s analyzer doesn\'t exist' %(analysis_type), 'loading':False}

def checkScrapedFinishedAndStartAnalysis(analysis_info, unique_id):
	try:
		with open('elements/gsicrawler-panel/analysis/%s.scraper'%unique_id, 'r') as analysis_file:
			scraped_reviews = json.loads(analysis_file.read())
			if('error' in scraped_reviews and scraped_reviews['error'] != None):
				return return_json(scraped_reviews)
			analysis_info['scraping'] = False
			analysis_types = analysis_info['analysis_type'].split(",")
			error = startAnalysis(scraped_reviews, analysis_types[0], unique_id)
			del analysis_types[0]
			analysis_info['analysis_type'] = ','.join(analysis_types)
			if(error):
				return return_json(error)
			return return_json(analysis_info)
	except Exception as e:
		print '######  checkScrapedFinishedAndStartAnalysis' + str(e)
		return return_json(analysis_info)

def checkAnalysisFinished(analysis_info, unique_id):
	print '##### ' +analysis_info['analysis_type']
	if(analysis_info['analysis_type'] == ''):
		try:
			analysis_result = None
			with open('elements/gsicrawler-panel/analysis/%s.analisis' % unique_id) as analysis_file:
				analysis_result = json.loads(analysis_file.read())
			os.remove('analysis/%s.analisis' % unique_id)
			os.remove('analysis/%s.scraper' % unique_id)
			return return_json(analysis_result)
		except Exception as e:
			return return_json(analysis_info)
	else:
		analysis_types = analysis_info['analysis_type'].split(",")
		try:
			analysis_result = None
			with open('elements/gsicrawler-panel/analysis/%s.analisis' % unique_id) as analysis_file:
				analysis_result = json.loads(analysis_file.read())
			if(analysis_result != None):
				os.remove('analysis/%s.analisis' % unique_id)
				startAnalysis(analysis_result, analysis_types[0], unique_id)
				del analysis_types[0]
				analysis_info['analysis_type'] = ','.join(analysis_types)
				return return_json(analysis_info)
		except Exception as e:
			return return_json(analysis_info)


@route('/')
def index():
	return static_file('index.html', root='front-end/app')

@route('/scrap_url')
def scrap_url():
	global scraping
	url = request.query['url']
	webpage = request.query['webpage']
	analysis_type = request.query['analysis_type']
	unique_id = str(uuid.uuid1())
	rv = {'error':None,  'loading':True, 'uuid':unique_id,
		  'analysis_type' : analysis_type,
		  'webpage':webpage, 'scraping':True}
	pending_analysis[unique_id] = rv
	startScraper(webpage, url, unique_id)
	return return_json(rv)

@route('/retrieve_info')
def retrieve_info():
	global pending_analysis, count
	unique_id = request.query['uuid']
	try:
		analysis_info = pending_analysis[unique_id]
		if(analysis_info['scraping']):
			return checkScrapedFinishedAndStartAnalysis(analysis_info, unique_id)
		else:
			return checkAnalysisFinished(analysis_info, unique_id)
	except Exception as e:
		print '#### retrieve_info ' + str(e)
		return return_json({'error':'No valid uuid', 'loading':False, 'uuid':unique_id})

@route('<filepath:path>')
def server_static(filepath):
	return static_file(filepath, root='/')

run(host='0.0.0.0', port=8000)
